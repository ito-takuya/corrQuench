{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical2a - Use Miller data set to demonstrate that during task states, correlated variability is quenched; compute average FR for each region separately\n",
    "## Using h5f data for faster i/o\n",
    "\n",
    "\n",
    "\n",
    "## Takuya Ito\n",
    "#### 08/01/2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import scipy.stats as stats\n",
    "import bct\n",
    "os.environ['OMP_NUM_THREADS'] = str(1)\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.sandbox.stats.multicomp as mc\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "plt.style.use(\"ggplot\")\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams[\"font.family\"] = \"FreeSans\"\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import sys\n",
    "import h5py\n",
    "import statsmodels.api as sm\n",
    "sys.path.append('../')\n",
    "import dimensionality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sessions with Paula: 39\n",
      "Number of sessions with Rex: 16\n"
     ]
    }
   ],
   "source": [
    "# sessions = ['100804']\n",
    "sessions = ['100706','100730','100804','100820','100827','100913','100921','101024','101122','101128',\n",
    "            '101207','101217','110110_01','110115_01','100724','100731','100817','100823','100828',\n",
    "            '100915','101008','101027','101123','101202','101209','110106','110110_02','110120','100725',\n",
    "            '100802','100818','100824','100907','100917','101009','101028','101124','101203','101210',\n",
    "            '110107_01','110111_01','110121','100726','100803','100819','100826','100910','100920','101023',\n",
    "            '101030','101127','101206','101216','110107_02','110111_02']\n",
    "# sessions = ['100706','100730','100804','100820','100827','100913','100921','101024','101122','101128',\n",
    "#             '101207','101217','110110_01','110115_01','100724','100731','100817','100823','100828']\n",
    "\n",
    "datadir = '/projects3/TaskFCMech/data/nhpData/'\n",
    "tmin = -4000 # in ms\n",
    "tmax = 4000 # in ms, this was my own doing\n",
    "\n",
    "# Load in monkeyIDs\n",
    "idfile = datadir + 'monkeyToSessionID.csv'\n",
    "monkeyTable = pd.read_csv(idfile,delimiter=',')\n",
    "\n",
    "sessionsWithPaula = 0\n",
    "monkeyID = {}\n",
    "for i in range(1, len(sessions)+1):\n",
    "    if i < 10:\n",
    "        sess_str = 'session_ ' + str(i)\n",
    "        name_str = 'name_ '  + str(i)\n",
    "    else:\n",
    "        sess_str = 'session_' + str(i)\n",
    "        name_str = 'name_'  + str(i)\n",
    "    \n",
    "    session = str(monkeyTable[sess_str][0])\n",
    "    name = monkeyTable[name_str][0]\n",
    "\n",
    "    if name=='paula':\n",
    "        sessionsWithPaula += 1\n",
    "        \n",
    "    if session=='110111_02.mat': session = '110111_02'\n",
    "        \n",
    "    monkeyID[session] = name\n",
    "    \n",
    "print 'Number of sessions with Paula:', sessionsWithPaula\n",
    "print 'Number of sessions with Rex:', len(sessions) - sessionsWithPaula\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create spike-triggered average response, across all rules/stims\n",
    "\n",
    "#### Use sliding window, 50ms sliding-window moving in 10ms (Churchland et al., 2010, Nat Neurosci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slidingWindow(data,binSize=50,shiftSize=10,nproc=10):\n",
    "    \"\"\"\n",
    "    data - organized by time (ms) x trial, data represents data from only one region/neuron\n",
    "    binsize - window size to compute number of spikes\n",
    "    shiftsize - shift window by this amount\n",
    "    \n",
    "    Effectively downsamples data\n",
    "    \"\"\"\n",
    "    \n",
    "    tLength = data.shape[0]\n",
    "    nTrials = data.shape[1]\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    inputs = []\n",
    "    for trial in range(nTrials):\n",
    "        inputs.append((data[:,trial],binSize,shiftSize))\n",
    "        \n",
    "    pool = mp.Pool(processes=nproc)\n",
    "    results = pool.map_async(_slide,inputs).get()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    out = []\n",
    "    for result in results:\n",
    "        out.append(result.T)\n",
    "        \n",
    "    outarray = np.zeros((result.shape[0],nTrials))\n",
    "    for i in range(nTrials):\n",
    "        outarray[:,i] = out[i]\n",
    "\n",
    "    return outarray\n",
    "\n",
    "### Helper function for parallel processing\n",
    "def _slide((trialdata,binSize,shiftSize)):\n",
    "    tLength = trialdata.shape[0]\n",
    "    \n",
    "    downSampledData = []\n",
    "    i = 0\n",
    "    while i < (tLength-binSize):\n",
    "        downSampledData.append(np.mean(trialdata[i:(i+binSize)],axis=0))\n",
    "        i += shiftSize\n",
    "\n",
    "    return np.asarray(downSampledData)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Load all sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load other meta-data associated with neurons + task info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute downsampled firing rate using 50ms bins and 10ms shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for session 100706 | 1 / 55\n",
      "Loading data for session 100730 | 2 / 55\n",
      "Loading data for session 100804 | 3 / 55\n",
      "Loading data for session 100820 | 4 / 55\n",
      "Loading data for session 100827 | 5 / 55\n",
      "Loading data for session 100913 | 6 / 55\n",
      "Loading data for session 100921 | 7 / 55\n",
      "Loading data for session 101024 | 8 / 55\n",
      "Loading data for session 101122 | 9 / 55\n",
      "Loading data for session 101128 | 10 / 55\n",
      "Loading data for session 101207 | 11 / 55\n",
      "Loading data for session 101217 | 12 / 55\n",
      "Loading data for session 110110_01 | 13 / 55\n",
      "Loading data for session 110115_01 | 14 / 55\n",
      "Loading data for session 100724 | 15 / 55\n",
      "Loading data for session 100731 | 16 / 55\n",
      "Loading data for session 100817 | 17 / 55\n",
      "Loading data for session 100823 | 18 / 55\n",
      "Loading data for session 100828 | 19 / 55\n",
      "Loading data for session 100915 | 20 / 55\n",
      "Loading data for session 101008 | 21 / 55\n",
      "Loading data for session 101027 | 22 / 55\n",
      "Loading data for session 101123 | 23 / 55\n",
      "Loading data for session 101202 | 24 / 55\n",
      "Loading data for session 101209 | 25 / 55\n",
      "Loading data for session 110106 | 26 / 55\n",
      "Loading data for session 110110_02 | 27 / 55\n",
      "Loading data for session 110120 | 28 / 55\n",
      "Loading data for session 100725 | 29 / 55\n",
      "Loading data for session 100802 | 30 / 55\n",
      "Loading data for session 100818 | 31 / 55\n",
      "Loading data for session 100824 | 32 / 55\n",
      "Loading data for session 100907 | 33 / 55\n",
      "Loading data for session 100917 | 34 / 55\n",
      "Loading data for session 101009 | 35 / 55\n",
      "Loading data for session 101028 | 36 / 55\n",
      "Loading data for session 101124 | 37 / 55\n",
      "Loading data for session 101203 | 38 / 55\n",
      "Loading data for session 101210 | 39 / 55\n",
      "Loading data for session 110107_01 | 40 / 55\n",
      "Loading data for session 110111_01 | 41 / 55\n",
      "Loading data for session 110121 | 42 / 55\n",
      "Loading data for session 100726 | 43 / 55\n",
      "Loading data for session 100803 | 44 / 55\n",
      "Loading data for session 100819 | 45 / 55\n",
      "Loading data for session 100826 | 46 / 55\n",
      "Loading data for session 100910 | 47 / 55\n",
      "Loading data for session 100920 | 48 / 55\n",
      "Loading data for session 101023 | 49 / 55\n",
      "Loading data for session 101030 | 50 / 55\n",
      "Loading data for session 101127 | 51 / 55\n",
      "Loading data for session 101206 | 52 / 55\n",
      "Loading data for session 101216 | 53 / 55\n",
      "Loading data for session 110107_02 | 54 / 55\n",
      "Loading data for session 110111_02 | 55 / 55\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "data['area'] = []\n",
    "data['session'] = []\n",
    "data['spikes'] = []\n",
    "data['spikesBinned'] = []\n",
    "data['STA_byTask'] = []\n",
    "data['taskInfo'] = []\n",
    "\n",
    "sessioncount = 1\n",
    "for session in sessions:\n",
    "    print 'Loading data for session', session, '|', sessioncount, '/', len(sessions)\n",
    "    \n",
    "    taskInfo = pd.read_csv(datadir + session + '_trialInfoAllTasks.csv')\n",
    "    areas = pd.read_csv(datadir + session + '_areaIndices.csv')\n",
    "\n",
    "    h5f = h5py.File(datadir + session + '_perArea.h5', 'r')\n",
    "    for i in range(len(areas)):\n",
    "        # Columns for h5f are reversed between matlab and python\n",
    "#         ind = np.where(networkdef==areas[i])[0]\n",
    "        data['area'].append(areas['Var1'][i])\n",
    "        data['session'].append(session)\n",
    "        data['spikes'].append(h5f['sta'][:,:,i].T)\n",
    "        spikes_binned = slidingWindow(h5f['sta'][:,:,i].T,binSize=50,shiftSize=10)\n",
    "        data['spikesBinned'].append(spikes_binned)\n",
    "        \n",
    "        tasksByTrial = taskInfo['task'].astype(str)\n",
    "        staByTask = {}\n",
    "        for task in np.unique(tasksByTrial):\n",
    "            task_ind = np.where(tasksByTrial==task)[0]\n",
    "            staByTask[task] = np.mean(spikes_binned[:,task_ind],axis=1)\n",
    "        data['STA_byTask'].append(staByTask)\n",
    "        data['taskInfo'].append(taskInfo)\n",
    "    h5f.close()\n",
    "    \n",
    "    sessioncount += 1\n",
    "    \n",
    "data = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSTA(df):\n",
    "    \"\"\"\n",
    "    Removes the task-specific STA from each trial (i.e., leaves the noise)\n",
    "    \"\"\"\n",
    "    sta_removed = []\n",
    "    for i in df.area.index:\n",
    "        tmpmat = df['spikesBinned'][i].copy()\n",
    "        for trial in range(tmpmat.shape[1]):\n",
    "            task = df['taskInfo'][i]['task'][trial]\n",
    "            tmpmat[:,trial] = tmpmat[:,trial] - df['STA_byTask'][i][task]\n",
    "        sta_removed.append(tmpmat)\n",
    "    sta_removed = np.asarray(sta_removed)\n",
    "        \n",
    "    return sta_removed\n",
    "    \n",
    "def computeStatistics(df,sta_removed):\n",
    "    \n",
    "    # Basic parameters\n",
    "    nCells = sta_removed.shape[0]\n",
    "    nTrials = sta_removed.shape[2]\n",
    "\n",
    "    session_ind = df.area.index[0]\n",
    "    \n",
    "    # Create empty arrays to store noise correlations\n",
    "    preStimNoiseCorr = np.zeros((nCells,nCells,nTrials))\n",
    "    postStimNoiseCorr = np.zeros((nCells,nCells,nTrials))\n",
    "    preCorr_avg = np.zeros((nTrials,))\n",
    "    postCorr_avg = np.zeros((nTrials,))\n",
    "\n",
    "    # Create empty arrays to store SD values\n",
    "    preStimNoiseSD = np.zeros((nCells,nTrials))\n",
    "    postStimNoiseSD = np.zeros((nCells,nTrials))\n",
    "\n",
    "    # Create empty arrays to store mean FR values\n",
    "    preStimFR = np.zeros((nCells,nTrials))\n",
    "    postStimFR = np.zeros((nCells,nTrials))\n",
    "\n",
    "    dimensionalityPre = np.zeros((nTrials,))\n",
    "    dimensionalityPost = np.zeros((nTrials,))\n",
    "\n",
    "    badTrials = []\n",
    "    badTrial1 = 0\n",
    "    badTrial2 = 0\n",
    "    for trial in range(nTrials):\n",
    "        # First identify the beginning of recording (prior to taskStart)\n",
    "        preStimStart = np.min(np.where(sta_removed[0,:,trial]!=0)[0])\n",
    "        try:\n",
    "            preStimEnd = np.max(np.where(time<(df['taskInfo'][session_ind]['fixptOn'][trial])*1000)[0]) # Convert trial start times to ms\n",
    "#             preStimEnd = np.max(np.where(time<(df['taskInfo'][session_ind]['trialStart'][trial])*1000)[0]) # Convert trial start times to ms\n",
    "    #         preStimEnd = np.max(np.where(time<(taskInfo['fixptOn'][trial])*1000)[0]) # Convert trial start times to ms\n",
    "        except:\n",
    "            badTrials.append(trial)\n",
    "            badTrial1 += 1\n",
    "            continue\n",
    "        nTPs = preStimEnd - preStimStart\n",
    "        if nTPs<75:\n",
    "            badTrials.append(trial)\n",
    "            badTrial2 += 1\n",
    "            continue\n",
    "#         # Task\n",
    "#         correct = df['taskInfo'][session_ind]['correct'][trial]\n",
    "#         if correct==0:\n",
    "#             badTrials.append(trial)\n",
    "#             badTrial2 += 1\n",
    "#             continue\n",
    "\n",
    "        postStimStart = np.min(np.where(time>=0)[0])\n",
    "#         postStimStart = np.max(np.where(time<(df['taskInfo'][session_ind]['fixptOn'][trial])*1000)[0]) # Convert trial start times to ms\n",
    "        postStimEnd = postStimStart + nTPs\n",
    "\n",
    "\n",
    "        # Noise correlation calculation\n",
    "        triu_ind = np.triu_indices(nCells,k=1)\n",
    "        tmp = np.zeros(sta_removed.shape)\n",
    "        tmp[:,preStimStart:postStimEnd,trial] = stats.zscore(sta_removed[:,preStimStart:postStimEnd,trial],axis=1)\n",
    "#         A = np.corrcoef(sta_removed[:,preStimStart:preStimEnd,trial])\n",
    "        A = np.corrcoef(tmp[:,preStimStart:preStimEnd,trial])\n",
    "        np.fill_diagonal(A,0)\n",
    "        preStimNoiseCorr[:,:,trial] = np.arctanh(A)\n",
    "        preCorr_avg[trial] = np.nanmean(np.arctanh(A))\n",
    "        dimensionalityPre[trial] = dimensionality.getDimensionality(np.corrcoef(sta_removed[:,preStimStart:preStimEnd,trial]))\n",
    "\n",
    "#         A = np.corrcoef(sta_removed[:,postStimStart:postStimEnd,trial])\n",
    "        A = np.corrcoef(tmp[:,postStimStart:postStimEnd,trial])\n",
    "        np.fill_diagonal(A,0)\n",
    "        postStimNoiseCorr[:,:,trial] = np.arctanh(A)\n",
    "        postCorr_avg[trial] = np.nanmean(np.arctanh(A))\n",
    "        dimensionalityPost[trial] = dimensionality.getDimensionality(np.corrcoef(sta_removed[:,postStimStart:postStimEnd,trial]))\n",
    "\n",
    "        # SD calculation\n",
    "        preStimNoiseSD[:,trial] = np.std(tmp[:,preStimStart:preStimEnd,trial],axis=1)\n",
    "        postStimNoiseSD[:,trial] = np.std(tmp[:,postStimStart:postStimEnd,trial],axis=1)\n",
    "#         preStimNoiseSD[:,trial] = np.std(sta_removed[:,preStimStart:preStimEnd,trial],axis=1)\n",
    "#         postStimNoiseSD[:,trial] = np.std(sta_removed[:,postStimStart:postStimEnd,trial],axis=1)\n",
    "\n",
    "        # FR calculation\n",
    "        i = 0\n",
    "        for area in df.area.index:\n",
    "            preStimFR[i,trial] = np.mean(df.spikesBinned[area][preStimStart:preStimEnd,trial],axis=0)\n",
    "            postStimFR[i,trial] = np.mean(df.spikesBinned[area][postStimStart:postStimEnd,trial],axis=0)\n",
    "            i += 1\n",
    "\n",
    "    badTrials = np.asarray(badTrials)\n",
    "    preStimNoiseCorr = np.delete(preStimNoiseCorr,badTrials,axis=2)\n",
    "    postStimNoiseCorr = np.delete(postStimNoiseCorr,badTrials,axis=2)\n",
    "    preCorr_avg = np.delete(preCorr_avg,badTrials,axis=0)\n",
    "    postCorr_avg = np.delete(postCorr_avg,badTrials,axis=0)\n",
    "    preStimNoiseSD = np.delete(preStimNoiseSD,badTrials,axis=1)\n",
    "    postStimNoiseSD = np.delete(postStimNoiseSD,badTrials,axis=1)\n",
    "    preStimFR = np.delete(preStimFR,badTrials,axis=1)\n",
    "    postStimFR = np.delete(postStimFR,badTrials,axis=1)\n",
    "    dimensionalityPre = np.delete(dimensionalityPre,badTrials,axis=0)\n",
    "    dimensionalityPost = np.delete(dimensionalityPost,badTrials,axis=0)\n",
    "    \n",
    "    dimReplications = {}\n",
    "    t, p = stats.ttest_rel(dimensionalityPost,dimensionalityPre)\n",
    "#     t, p = stats.wilcoxon(dimensionalityPost,dimensionalityPre)\n",
    "    dimReplications['avg_post'] = np.mean(dimensionalityPost)\n",
    "    dimReplications['avg_pre'] = np.mean(dimensionalityPre)\n",
    "    dimReplications['t'] = t\n",
    "    p=p/2.0 if t>0 else 1.0-p/2.0\n",
    "    dimReplications['p'] = p\n",
    "\n",
    "    spkCorrReplications = {}\n",
    "    t, p = stats.ttest_rel(postCorr_avg,preCorr_avg)\n",
    "#     t, p = stats.wilcoxon(postCorr_avg,preCorr_avg)\n",
    "    spkCorrReplications['avg_post'] = np.mean(postCorr_avg)\n",
    "    spkCorrReplications['avg_pre'] = np.mean(preCorr_avg)\n",
    "    spkCorrReplications['t'] = t\n",
    "    p=p/2.0 if t<0 else 1.0-p/2.0\n",
    "    spkCorrReplications['p'] = p\n",
    "    \n",
    "    sdReplications = {}\n",
    "    t, p = stats.ttest_rel(np.mean(postStimNoiseSD,0),np.mean(preStimNoiseSD,axis=0))\n",
    "#     t, p = stats.wilcoxon(np.mean(postStimNoiseSD,0),np.mean(preStimNoiseSD,axis=0))\n",
    "    sdReplications['avg_post'] = np.mean(postStimNoiseSD)\n",
    "    sdReplications['avg_pre'] = np.mean(preStimNoiseSD)\n",
    "    sdReplications['t'] = t\n",
    "    p=p/2.0 if t<0 else 1.0-p/2.0\n",
    "    sdReplications['p'] = p\n",
    "    \n",
    "    frReplications = {}\n",
    "    t, p = stats.ttest_rel(np.mean(postStimFR,0),np.mean(preStimFR,axis=0))\n",
    "#     t, p = stats.wilcoxon(np.mean(postStimFR,0),np.mean(preStimFR,axis=0))\n",
    "    frReplications['avg_post'] = np.mean(postStimFR)\n",
    "    frReplications['avg_pre'] = np.mean(preStimFR)\n",
    "    frReplications['t'] = t\n",
    "    p=p/2.0 if t>0 else 1.0-p/2.0\n",
    "    frReplications['p'] = p\n",
    "    \n",
    "#     print 'Total number of bad Trials:', badTrial1 + badTrial2, '/', nTrials\n",
    "#     print '\\tNumber of Bad Trials 1', badTrial1\n",
    "#     print '\\tNumber of Bad Trials 2', badTrial2\n",
    "\n",
    "    return dimReplications, spkCorrReplications, sdReplications, frReplications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform main analyses for Paula only first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics for session 1 / 39\n",
      "Computing statistics for session 2 / 39\n",
      "Computing statistics for session 3 / 39\n",
      "Computing statistics for session 4 / 39\n",
      "Computing statistics for session 5 / 39\n",
      "Computing statistics for session 6 / 39\n",
      "Computing statistics for session 7 / 39\n",
      "Computing statistics for session 8 / 39\n",
      "Computing statistics for session 9 / 39\n",
      "Computing statistics for session 10 / 39\n",
      "Computing statistics for session 11 / 39\n",
      "Computing statistics for session 12 / 39\n",
      "Computing statistics for session 13 / 39\n",
      "Computing statistics for session 14 / 39\n",
      "Computing statistics for session 15 / 39\n",
      "Computing statistics for session 16 / 39\n",
      "Computing statistics for session 17 / 39\n",
      "Computing statistics for session 18 / 39\n",
      "Computing statistics for session 19 / 39\n",
      "Computing statistics for session 20 / 39\n",
      "Computing statistics for session 21 / 39\n",
      "Computing statistics for session 22 / 39\n",
      "Computing statistics for session 23 / 39\n",
      "Computing statistics for session 24 / 39\n",
      "Computing statistics for session 25 / 39\n",
      "Computing statistics for session 26 / 39\n",
      "Computing statistics for session 27 / 39\n",
      "Computing statistics for session 28 / 39\n",
      "Computing statistics for session 29 / 39\n",
      "Computing statistics for session 30 / 39\n",
      "Computing statistics for session 31 / 39\n",
      "Computing statistics for session 32 / 39\n",
      "Computing statistics for session 33 / 39\n",
      "Computing statistics for session 34 / 39\n",
      "Computing statistics for session 35 / 39\n",
      "Computing statistics for session 36 / 39\n",
      "Computing statistics for session 37 / 39\n",
      "Computing statistics for session 38 / 39\n",
      "Computing statistics for session 39 / 39\n",
      "\n",
      "\n",
      "**RESULTS**\n",
      "Dimensionality results replicated: 38 / 39\n",
      "SpkCorr results replicated: 31 / 39\n",
      "SD results replicated: 26 / 39\n",
      "FR results replicated: 21 / 39\n"
     ]
    }
   ],
   "source": [
    "dimReplications = {}\n",
    "dimReplications['avg_post'] = []\n",
    "dimReplications['avg_pre'] = []\n",
    "dimReplications['t'] = []\n",
    "dimReplications['p'] = []\n",
    "\n",
    "spkCorrReplications = {}\n",
    "spkCorrReplications['avg_post'] = []\n",
    "spkCorrReplications['avg_pre'] = []\n",
    "spkCorrReplications['t'] = []\n",
    "spkCorrReplications['p'] = []\n",
    "\n",
    "sdReplications = {}\n",
    "sdReplications['avg_post'] = []\n",
    "sdReplications['avg_pre'] = []\n",
    "sdReplications['t'] = []\n",
    "sdReplications['p'] = []\n",
    "\n",
    "frReplications = {}\n",
    "frReplications['avg_post'] = []\n",
    "frReplications['avg_pre'] = []\n",
    "frReplications['t'] = []\n",
    "frReplications['p'] = []\n",
    "\n",
    "\n",
    "totalDims = []\n",
    "totalSpkCorr = []\n",
    "totalSD = []\n",
    "totalFR = []\n",
    "sesscount = 1\n",
    "for session in sessions:\n",
    "    if monkeyID[session]=='rex': continue\n",
    "    print 'Computing statistics for session', sesscount, '/', sessionsWithPaula\n",
    "    df = data.loc[data['session']==session]\n",
    "    sta_removed = removeSTA(df)\n",
    "    time = np.linspace(tmin,tmax,sta_removed.shape[1])\n",
    "    tmp1, tmp2, tmp3, tmp4 = computeStatistics(df,sta_removed)\n",
    "    \n",
    "    dimReplications['avg_post'].append(tmp1['avg_post'])\n",
    "    dimReplications['avg_pre'].append(tmp1['avg_pre'])\n",
    "    dimReplications['t'].append(tmp1['t'])\n",
    "    dimReplications['p'].append(tmp1['p'])\n",
    "    \n",
    "    spkCorrReplications['avg_post'].append(tmp2['avg_post'])\n",
    "    spkCorrReplications['avg_pre'].append(tmp2['avg_pre'])\n",
    "    spkCorrReplications['t'].append(tmp2['t'])\n",
    "    spkCorrReplications['p'].append(tmp2['p'])\n",
    "    \n",
    "    sdReplications['avg_post'].append(tmp3['avg_post'])\n",
    "    sdReplications['avg_pre'].append(tmp3['avg_pre'])\n",
    "    sdReplications['t'].append(tmp3['t'])\n",
    "    sdReplications['p'].append(tmp3['p'])\n",
    "    \n",
    "    frReplications['avg_post'].append(tmp4['avg_post'])\n",
    "    frReplications['avg_pre'].append(tmp4['avg_pre'])\n",
    "    frReplications['t'].append(tmp4['t'])\n",
    "    frReplications['p'].append(tmp4['p'])\n",
    "    \n",
    "    if tmp1['t'] > 0 and tmp1['p'] < 0.05:\n",
    "        totalDims.append(True)\n",
    "    else:\n",
    "        totalDims.append(False)\n",
    "        \n",
    "    if tmp2['t'] < 0 and tmp2['p'] < 0.05:\n",
    "        totalSpkCorr.append(True)\n",
    "    else:\n",
    "        totalSpkCorr.append(False)\n",
    "        \n",
    "    if tmp3['t'] < 0 and tmp3['p'] < 0.05:\n",
    "        totalSD.append(True)\n",
    "    else:\n",
    "        totalSD.append(False)\n",
    "    \n",
    "    if tmp4['t'] > 0 and tmp4['p'] < 0.05:\n",
    "        totalFR.append(True)\n",
    "    else:\n",
    "        totalFR.append(False)\n",
    "        \n",
    "    sesscount += 1\n",
    "    \n",
    "print '\\n'\n",
    "print '**RESULTS**'\n",
    "print 'Dimensionality results replicated:', np.sum(totalDims), '/', len(totalDims)\n",
    "print 'SpkCorr results replicated:', np.sum(totalSpkCorr), '/', len(totalSpkCorr)\n",
    "print 'SD results replicated:', np.sum(totalSD), '/', len(totalSD)\n",
    "print 'FR results replicated:', np.sum(totalFR), '/', len(totalFR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute statistics for Paula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality Post- vs. Pre-stim:\n",
      "\tt = 8.533 | p = 2.3085254385e-10\n",
      "SpikeCountCorr Post- vs. Pre-stim:\n",
      "\tt = -6.244 | p = 2.63102651843e-07\n",
      "SD variability Post- vs. Pre-stim:\n",
      "\tt = -4.247 | p = 0.0001348645474\n",
      "Firing rate Post- vs. Pre-stim:\n",
      "\tt = 3.337 | p = 0.00190143761331\n"
     ]
    }
   ],
   "source": [
    "t, p = stats.ttest_rel(dimReplications['avg_post'],dimReplications['avg_pre'])\n",
    "print 'Dimensionality Post- vs. Pre-stim:'\n",
    "print '\\tt =', round(t,3), '| p =', p\n",
    "\n",
    "t, p = stats.ttest_rel(spkCorrReplications['avg_post'],spkCorrReplications['avg_pre'])\n",
    "print 'SpikeCountCorr Post- vs. Pre-stim:'\n",
    "print '\\tt =', round(t,3), '| p =', p\n",
    "\n",
    "t, p = stats.ttest_rel(sdReplications['avg_post'],sdReplications['avg_pre'])\n",
    "print 'SD variability Post- vs. Pre-stim:'\n",
    "print '\\tt =', round(t,3), '| p =', p\n",
    "\n",
    "t, p = stats.ttest_rel(frReplications['avg_post'],frReplications['avg_pre'])\n",
    "print 'Firing rate Post- vs. Pre-stim:'\n",
    "print '\\tt =', round(t,3), '| p =', p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform main analyses for Rex as replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics for session 1 / 16\n",
      "Computing statistics for session 2 / 16\n",
      "Computing statistics for session 3 / 16\n",
      "Computing statistics for session 4 / 16\n",
      "Computing statistics for session 5 / 16\n",
      "Computing statistics for session 6 / 16\n",
      "Computing statistics for session 7 / 16\n",
      "Computing statistics for session 8 / 16\n",
      "Computing statistics for session 9 / 16\n",
      "Computing statistics for session 10 / 16\n",
      "Computing statistics for session 11 / 16\n",
      "Computing statistics for session 12 / 16\n",
      "Computing statistics for session 13 / 16\n",
      "Computing statistics for session 14 / 16\n",
      "Computing statistics for session 15 / 16\n",
      "Computing statistics for session 16 / 16\n",
      "\n",
      "\n",
      "**RESULTS**\n",
      "Dimensionality results replicated: 16 / 16\n",
      "SpkCorr results replicated: 16 / 16\n",
      "SD results replicated: 16 / 16\n",
      "FR results replicated: 16 / 16\n"
     ]
    }
   ],
   "source": [
    "dimReplications = {}\n",
    "dimReplications['avg_post'] = []\n",
    "dimReplications['avg_pre'] = []\n",
    "dimReplications['t'] = []\n",
    "dimReplications['p'] = []\n",
    "\n",
    "spkCorrReplications = {}\n",
    "spkCorrReplications['avg_post'] = []\n",
    "spkCorrReplications['avg_pre'] = []\n",
    "spkCorrReplications['t'] = []\n",
    "spkCorrReplications['p'] = []\n",
    "\n",
    "sdReplications = {}\n",
    "sdReplications['avg_post'] = []\n",
    "sdReplications['avg_pre'] = []\n",
    "sdReplications['t'] = []\n",
    "sdReplications['p'] = []\n",
    "\n",
    "frReplications = {}\n",
    "frReplications['avg_post'] = []\n",
    "frReplications['avg_pre'] = []\n",
    "frReplications['t'] = []\n",
    "frReplications['p'] = []\n",
    "\n",
    "\n",
    "totalDims = []\n",
    "totalSpkCorr = []\n",
    "totalSD = []\n",
    "totalFR = []\n",
    "sesscount = 1\n",
    "for session in sessions:\n",
    "    if monkeyID[session]=='paula': continue\n",
    "    print 'Computing statistics for session', sesscount, '/', len(sessions) - sessionsWithPaula\n",
    "    df = data.loc[data['session']==session]\n",
    "    sta_removed = removeSTA(df)\n",
    "    time = np.linspace(tmin,tmax,sta_removed.shape[1])\n",
    "    tmp1, tmp2, tmp3, tmp4 = computeStatistics(df,sta_removed)\n",
    "    \n",
    "    dimReplications['avg_post'].append(tmp1['avg_post'])\n",
    "    dimReplications['avg_pre'].append(tmp1['avg_pre'])\n",
    "    dimReplications['t'].append(tmp1['t'])\n",
    "    dimReplications['p'].append(tmp1['p'])\n",
    "    \n",
    "    spkCorrReplications['avg_post'].append(tmp2['avg_post'])\n",
    "    spkCorrReplications['avg_pre'].append(tmp2['avg_pre'])\n",
    "    spkCorrReplications['t'].append(tmp2['t'])\n",
    "    spkCorrReplications['p'].append(tmp2['p'])\n",
    "    \n",
    "    sdReplications['avg_post'].append(tmp3['avg_post'])\n",
    "    sdReplications['avg_pre'].append(tmp3['avg_pre'])\n",
    "    sdReplications['t'].append(tmp3['t'])\n",
    "    sdReplications['p'].append(tmp3['p'])\n",
    "    \n",
    "    frReplications['avg_post'].append(tmp4['avg_post'])\n",
    "    frReplications['avg_pre'].append(tmp4['avg_pre'])\n",
    "    frReplications['t'].append(tmp4['t'])\n",
    "    frReplications['p'].append(tmp4['p'])\n",
    "    \n",
    "    if tmp1['t'] > 0 and tmp1['p'] < 0.05:\n",
    "        totalDims.append(True)\n",
    "    else:\n",
    "        totalDims.append(False)\n",
    "        \n",
    "    if tmp2['t'] < 0 and tmp2['p'] < 0.05:\n",
    "        totalSpkCorr.append(True)\n",
    "    else:\n",
    "        totalSpkCorr.append(False)\n",
    "        \n",
    "    if tmp3['t'] < 0 and tmp3['p'] < 0.05:\n",
    "        totalSD.append(True)\n",
    "    else:\n",
    "        totalSD.append(False)\n",
    "    \n",
    "    if tmp4['t'] > 0 and tmp4['p'] < 0.05:\n",
    "        totalFR.append(True)\n",
    "    else:\n",
    "        totalFR.append(False)\n",
    "        \n",
    "    sesscount += 1\n",
    "    \n",
    "print '\\n'\n",
    "print '**RESULTS**'\n",
    "print 'Dimensionality results replicated:', np.sum(totalDims), '/', len(totalDims)\n",
    "print 'SpkCorr results replicated:', np.sum(totalSpkCorr), '/', len(totalSpkCorr)\n",
    "print 'SD results replicated:', np.sum(totalSD), '/', len(totalSD)\n",
    "print 'FR results replicated:', np.sum(totalFR), '/', len(totalFR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute statistics for Rex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality Post- vs. Pre-stim:\n",
      "\tt = 7.772 | p = 1.22698820533e-06\n",
      "SpikeCountCorr Post- vs. Pre-stim:\n",
      "\tt = -10.221 | p = 3.74459173513e-08\n",
      "SD variability Post- vs. Pre-stim:\n",
      "\tt = -11.721 | p = 5.95466044022e-09\n",
      "Firing rate Post- vs. Pre-stim:\n",
      "\tt = 6.612 | p = 8.2610631857e-06\n"
     ]
    }
   ],
   "source": [
    "t, p = stats.ttest_rel(dimReplications['avg_post'],dimReplications['avg_pre'])\n",
    "print 'Dimensionality Post- vs. Pre-stim:'\n",
    "print '\\tt =', round(t,3), '| p =', p\n",
    "\n",
    "t, p = stats.ttest_rel(spkCorrReplications['avg_post'],spkCorrReplications['avg_pre'])\n",
    "print 'SpikeCountCorr Post- vs. Pre-stim:'\n",
    "print '\\tt =', round(t,3), '| p =', p\n",
    "\n",
    "t, p = stats.ttest_rel(sdReplications['avg_post'],sdReplications['avg_pre'])\n",
    "print 'SD variability Post- vs. Pre-stim:'\n",
    "print '\\tt =', round(t,3), '| p =', p\n",
    "\n",
    "t, p = stats.ttest_rel(frReplications['avg_post'],frReplications['avg_pre'])\n",
    "print 'Firing rate Post- vs. Pre-stim:'\n",
    "print '\\tt =', round(t,3), '| p =', p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
